{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46984f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "\n",
    "#august\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#!pip install tensorflow\n",
    "import tensorflow\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b156ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Image(filename='brad.jpg'))\n",
    "#display(Image(filename='kim.jpg'))\n",
    "face_cascade = cv.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv.CascadeClassifier(\"haarcascade_eye.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c7241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_for_crop(img_address):\n",
    "\n",
    "    img = cv.imread(img_address)\n",
    "    im = Image.open(img_address) \n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "    xUpper, yUpper, xLower, yLower = 0,0,0,0\n",
    "    \n",
    "    \n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        #print(w,h,type(w),type(h))\n",
    "        if (w>200 and h>200):\n",
    "            return x,y,x+w,y+h\n",
    "        \n",
    "        \n",
    "        #cv.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        #roi_gray = gray[y : y + h, x : x + h]\n",
    "        #roi_color = img[y : y + h, x : x + h]\n",
    "        # Otsi silmi vaid leitud nägude aladelt (kollased ristkülikud)\n",
    "        #eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        #for (ex, ey, ew, eh) in eyes:\n",
    "            #cv.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 255), 2)\n",
    "\n",
    "\n",
    "    # Otsi silmi kogu fotolt (rohelised ristkülikud)\n",
    "    #eyes = eye_cascade.detectMultiScale(gray)\n",
    "    #for (ex, ey, ew, eh) in eyes:\n",
    "        #cv.rectangle(img, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "\n",
    "    #im1 = im.crop((xUpper, yUpper, xLower, yLower))\n",
    "    #im1.show()\n",
    "    #cv.waitKey(0)\n",
    "    #cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999f0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pildi salvestamiseks\n",
    "#cv.imwrite('jott_koopia.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77570e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce3379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7718eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 210 entries, 0 to 209\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   usaldusväärsus  180 non-null    float64\n",
      " 1   ilu             180 non-null    float64\n",
      " 2   intelligents    180 non-null    float64\n",
      " 3   pilt            210 non-null    object \n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 6.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "# Defineerime CSV faili tee ja piltide peamise kausta tee\n",
    "csv_file_path = './ratings.csv'\n",
    "images_folder_path = './people'\n",
    "\n",
    "# Laadime CSV faili\n",
    "df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "# Loome tühja DataFrame, kuhu salvestame ümberstruktureeritud andmed\n",
    "columns = ['usaldusväärsus', 'ilu', 'intelligents', \"pilt\"]\n",
    "andmestik_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Itereerime läbi CSV andmete ja lisame need ümberstruktureeritult uude DataFrame'i\n",
    "for index, row in df.iterrows():\n",
    "    # Määrame kausta nime vastavalt rea indeksile\n",
    "    folder_name = f'id{index + 1}'\n",
    "    folder_path = os.path.join(images_folder_path, folder_name)\n",
    "    \n",
    "    # Lisame andmed vastavalt kaustas olevatele piltidele\n",
    "    for i in range(0, len(row)-3, 3):\n",
    "        image_name = f'Allalaaditud fail ({i//3 + 1}).jpeg'  # Oletame, et pildid on nimedega img1.jpg, img2.jpg, img3.jpg\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        \n",
    "        # load the image and convert into\n",
    "        # numpy array\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        #crop image so it will only contain the face of the person\n",
    "        img = img.crop(coords_for_crop(image_path))\n",
    "        \n",
    "        #resize image to 600x600 \n",
    "        img = img.resize((600,600))\n",
    "        \n",
    "        ##\n",
    "        #img.show()\n",
    "        ##\n",
    "        \n",
    "        # asarray() class is used to convert\n",
    "        # PIL images into NumPy arrays\n",
    "        image_array = asarray(img)\n",
    "        \n",
    "        \n",
    "        image_data = pd.DataFrame({\n",
    "            'usaldusväärsus': [row[i]],\n",
    "            'ilu': [row[i+1]],\n",
    "            'intelligents': [row[i+2]],\n",
    "            'pilt': [image_array]  \n",
    "        })\n",
    "        andmestik_df = pd.concat([andmestik_df,image_data], ignore_index=True)\n",
    "\n",
    "\n",
    "# Kui soovite tulemused salvestada uude CSV faili\n",
    "andmestik_df.to_csv('reshaped_file.csv', index=True)\n",
    "# Vaatame ümberstruktureeritud andmeid\n",
    "print(andmestik_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbaa937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93812f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kost\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 6s/step - loss: 6487.5068 - mae: 43.1250 - val_loss: 22.6165 - val_mae: 4.4275\n",
      "Epoch 2/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - loss: 12.2090 - mae: 2.9062 - val_loss: 5.7639 - val_mae: 2.0483\n",
      "Epoch 3/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7s/step - loss: 7.3075 - mae: 2.2267 - val_loss: 5.1407 - val_mae: 1.9980\n",
      "Epoch 4/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 5s/step - loss: 5.3143 - mae: 1.8987 - val_loss: 4.1208 - val_mae: 1.7628\n",
      "Epoch 5/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 5s/step - loss: 5.0762 - mae: 1.8164 - val_loss: 3.4902 - val_mae: 1.4383\n",
      "Epoch 6/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 6s/step - loss: 5.9094 - mae: 2.0152 - val_loss: 3.7966 - val_mae: 1.6700\n",
      "Epoch 7/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6s/step - loss: 4.9457 - mae: 1.8101 - val_loss: 3.9803 - val_mae: 1.7324\n",
      "Epoch 8/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 6s/step - loss: 4.8861 - mae: 1.8457 - val_loss: 5.8209 - val_mae: 2.1135\n",
      "Epoch 9/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 6s/step - loss: 5.6291 - mae: 1.9959 - val_loss: 4.5498 - val_mae: 1.8555\n",
      "Epoch 10/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 5s/step - loss: 5.3077 - mae: 1.9086 - val_loss: 4.9044 - val_mae: 1.9028\n",
      "Epoch 11/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 6s/step - loss: 4.5458 - mae: 1.7473 - val_loss: 3.1465 - val_mae: 1.3814\n",
      "Epoch 12/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 5s/step - loss: 4.8660 - mae: 1.7559 - val_loss: 3.7098 - val_mae: 1.6688\n",
      "Epoch 13/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5s/step - loss: 3.9048 - mae: 1.5690 - val_loss: 3.5463 - val_mae: 1.5830\n",
      "Epoch 14/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 5s/step - loss: 3.7515 - mae: 1.5688 - val_loss: 4.2779 - val_mae: 1.7461\n",
      "Epoch 15/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - loss: 4.1599 - mae: 1.6185 - val_loss: 5.1335 - val_mae: 1.9406\n",
      "Epoch 16/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - loss: 4.3742 - mae: 1.7032 - val_loss: 4.8493 - val_mae: 1.9120\n",
      "Epoch 17/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - loss: 4.1537 - mae: 1.6591 - val_loss: 3.5939 - val_mae: 1.6294\n",
      "Epoch 18/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5s/step - loss: 3.9947 - mae: 1.6186 - val_loss: 3.6610 - val_mae: 1.5865\n",
      "Epoch 19/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - loss: 3.3204 - mae: 1.4190 - val_loss: 3.5994 - val_mae: 1.5462\n",
      "Epoch 20/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 6s/step - loss: 3.1309 - mae: 1.4190 - val_loss: 3.1429 - val_mae: 1.4536\n",
      "Epoch 21/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m845s\u001b[0m 119s/step - loss: 3.1565 - mae: 1.4045 - val_loss: 3.8101 - val_mae: 1.5937\n",
      "Epoch 22/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - loss: 3.9908 - mae: 1.6187 - val_loss: 3.3664 - val_mae: 1.5303\n",
      "Epoch 23/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5s/step - loss: 3.3516 - mae: 1.4615 - val_loss: 3.7480 - val_mae: 1.6593\n",
      "Epoch 24/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - loss: 2.8541 - mae: 1.3360 - val_loss: 3.6896 - val_mae: 1.6122\n",
      "Epoch 25/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 6s/step - loss: 2.0916 - mae: 1.1334 - val_loss: 3.5187 - val_mae: 1.5551\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528ms/step - loss: 4.7379 - mae: 1.8295\n",
      "Model evaluation - Loss: 4.726557731628418, MAE: 1.832613229751587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "#df = pd.read_csv('reshaped_file.csv')  # assuming your data is in a CSV file\n",
    "df = andmestik_df\n",
    "\n",
    "# Drop rows with missing ratings\n",
    "df = df.dropna(subset=['usaldusväärsus', 'ilu', 'intelligents'])\n",
    "\n",
    "# Extract the images and labels\n",
    "images = np.stack(df['pilt'].values)\n",
    "ratings = df[['usaldusväärsus', 'ilu', 'intelligents']].values\n",
    "\n",
    "# Normalize image data\n",
    "images = images / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(600, 600, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3)  # Output layer for the 3 ratings\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Model evaluation - Loss: {loss}, MAE: {mae}')\n",
    "\n",
    "# Save the model\n",
    "model.save('face_ratings_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a9bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f14c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = load_model('face_ratings_model.h5')\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    #image = image.crop(coords_for_crop(image_path))\n",
    "    image = image.resize((600, 600))  # Ensure the image is 600x600 pixels\n",
    "    image.show()\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "\n",
    "def display_results(image_path):\n",
    "    # Path to the random image\n",
    "    random_image_path = image_path  # Replace with the actual path\n",
    "\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(random_image_path)\n",
    "\n",
    "    # Predict the ratings\n",
    "    predicted_ratings = model.predict(preprocessed_image)\n",
    "\n",
    "    # Print the predicted ratings\n",
    "    usaldusvaarus, ilu, intelligents = predicted_ratings[0]\n",
    "    print(f\"Predicted Reliability (usaldusväärsus): {usaldusvaarus}\")\n",
    "    print(f\"Predicted Attractiveness (ilu): {ilu}\")\n",
    "    print(f\"Predicted Intelligence (intelligents): {intelligents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ff89c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyler ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "Predicted Reliability (usaldusväärsus): 4.001458644866943\n",
      "Predicted Attractiveness (ilu): 4.326530456542969\n",
      "Predicted Intelligence (intelligents): 4.633413314819336\n",
      "kim ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "Predicted Reliability (usaldusväärsus): 4.8442301750183105\n",
      "Predicted Attractiveness (ilu): 7.144835948944092\n",
      "Predicted Intelligence (intelligents): 7.220454692840576\n",
      "brad ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "Predicted Reliability (usaldusväärsus): 7.994103908538818\n",
      "Predicted Attractiveness (ilu): 8.526636123657227\n",
      "Predicted Intelligence (intelligents): 7.176937580108643\n",
      "homeless ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Predicted Reliability (usaldusväärsus): 3.338634490966797\n",
      "Predicted Attractiveness (ilu): 4.160274505615234\n",
      "Predicted Intelligence (intelligents): 4.202895164489746\n"
     ]
    }
   ],
   "source": [
    "print(\"tyler ratings:\")\n",
    "display_results('people/tyler.jpg')\n",
    "\n",
    "print(\"kim ratings:\")\n",
    "display_results('people/kim.jpg')\n",
    "\n",
    "print(\"brad ratings:\")\n",
    "display_results('people/brad.jpg')\n",
    "\n",
    "print(\"homeless ratings:\")\n",
    "display_results('people/homeless.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6dfe00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just some dude ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step\n",
      "Predicted Reliability (usaldusväärsus): 6.090631484985352\n",
      "Predicted Attractiveness (ilu): 6.4580159187316895\n",
      "Predicted Intelligence (intelligents): 6.231784820556641\n",
      "man ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Predicted Reliability (usaldusväärsus): 4.635649681091309\n",
      "Predicted Attractiveness (ilu): 5.2997660636901855\n",
      "Predicted Intelligence (intelligents): 5.339832305908203\n",
      "airbrushed face ratings:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Predicted Reliability (usaldusväärsus): 5.652440071105957\n",
      "Predicted Attractiveness (ilu): 5.758391857147217\n",
      "Predicted Intelligence (intelligents): 5.533234119415283\n"
     ]
    }
   ],
   "source": [
    "print(\"just some dude ratings:\")\n",
    "display_results('people/girl.png')\n",
    "\n",
    "print(\"man ratings:\")\n",
    "display_results('people/old_man.png')\n",
    "\n",
    "print(\"airbrushed face ratings:\")\n",
    "display_results('people/airbrushed.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
